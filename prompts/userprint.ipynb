{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7779c450-a94a-45cd-b9a5-e34ef083880c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa1de30cdd5432e9ea97b479692a420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00006-4feeb3f83346a0e9.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea037ded8c24f2184623401fe075fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00001-of-00006-4030672591c2f478.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e84f76f34e64a30870af720c9d53a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00002-of-00006-1779b7cec9462180.parquet:   0%|          | 0.00/250M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95259cf4233e49029e30dce1d81f72c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00003-of-00006-2fa862bfed56af1f.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f7c83b64e54173a154dde0127cf595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00004-of-00006-18f4bdd50c103e71.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ad81245b5f43a099532aa6288f1cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00005-of-00006-fe1acc5d10a9f0e2.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac56ad480a8c4b0f9c88916d1c54e094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = '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'  \n",
    "\n",
    "# Then load the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"lmsys/lmsys-chat-1m\")['train']\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77a418b3-122e-4116-b97f-9e26288d12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english_clean(example):\n",
    "    try:\n",
    "        lang = example['language']\n",
    "        flagged = example['openai_moderation'][0].get('flagged', False) if example['openai_moderation'] else True\n",
    "        return (\n",
    "            lang == 'English' and\n",
    "            not example['redacted'] and\n",
    "            not flagged\n",
    "        )\n",
    "    except:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25866e3-eb03-4023-be42-387af83a5957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b449322a2bb34e2daedfbda3ec169b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497355\n"
     ]
    }
   ],
   "source": [
    "filtered_ds = dataset.filter(filter_english_clean)\n",
    "print(len(filtered_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8cb9f50-e1ec-46e8-a456-412746e22071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763597561df14f2da2749c71a3ffa190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/497355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can identity protection services help protect me against identity theft\n"
     ]
    }
   ],
   "source": [
    "def extract_user_text(example):\n",
    "    user_msgs = [msg['content'] for msg in example['conversation'] if msg['role'] == 'user']\n",
    "    return {'user_text': ' '.join(user_msgs)}\n",
    "\n",
    "filtered_ds = filtered_ds.map(extract_user_text)\n",
    "print(filtered_ds[0]['user_text']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7b525c-773f-4da9-84d7-347af4d7eb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9037ae488d14fec900fae80237b636f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Use a smaller slice for now (e.g. first 10K) to test\n",
    "user_texts = filtered_ds['user_text'][:10000]\n",
    "embeddings = model.encode(user_texts, batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Convert embeddings to a list of numpy arrays (each array is 1D)\n",
    "embeddings_list = [np.array(embedding) for embedding in embeddings]\n",
    "\n",
    "# Add to dataset\n",
    "filtered_slice = filtered_ds.select(range(10000)).add_column(\"embedding\", embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38aecbce-3d20-4e60-a477-84997d09c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 10  # you can tune this\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Add cluster labels to dataset\n",
    "filtered_slice = filtered_slice.add_column(\"persona_cluster\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04ed811-98c3-4da5-87fa-c3242f85205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Cluster 7 Sample Persona:\n",
      "What is the purpose of grief? I mean evolutionarily Can you do ascii art? Go for it What are the health effects of uv resin? is 405nm light hazardous in any way? Explain tensor cores Why exactly are they better at those tasks? does that make then ASICs? What causes cell senescence?\n",
      "\n",
      "\n",
      "üß† Cluster 1 Sample Persona:\n",
      "You are the text completion model and you must complete the assistant answer below, only send the completion based on the system instructions.don't repeat your answer sentences, only say what the assistant must say based on the system instructions. repeating same thing in same answer not allowed.\n",
      "user: descriptive answer for how to send a message from google form to a python in python with proper code examples and outputs.\n",
      "assistant: \n",
      "\n",
      "\n",
      "üß† Cluster 8 Sample Persona:\n",
      "The planet Goob completes one revolution after every 2 weeks. How many hours will it take for it to complete half a revolution? There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
      "\n",
      "\n",
      "üß† Cluster 0 Sample Persona:\n",
      "can you plan the las vegas trip ?\n",
      "\n",
      "\n",
      "üß† Cluster 9 Sample Persona:\n",
      "Given the document below, you have to determine if \"Yes\" or \"No\", the summary is factually consistent with the document.\n",
      "\n",
      "Document:\n",
      "We are also raising our adjusted earnings per share range for the full year by $0.30 per share and now expect adjusted earnings per share of $3.67 per share, at the midpoint of the range provided. We are raising our full-year guidance range for net sales, operating EBITDA and adjusted EPS. Shelter solutions had low single-digit organic growth versus the year ago period, reflecting high single-digit organic growth in residential construction and retail channels for do-it-yourself application, offset partially by softness in commercial construction market. We expect to improve leverage and now expect operating EBITDA for the year to be about 4.03 billion, at the midpoint of the range provided, a year-over-year increase of 17%. This supply situation is gradually improving while we anticipate several critical products will continue to constrain our production \n",
      "\n",
      "\n",
      "üß† Cluster 3 Sample Persona:\n",
      "Give me an introduction over 200 words for panjin Hengchanglong Refined Chemical.Co.,Ltd , a chemical company in rm308 office building,development zone,panjin city,liaoning province,china China\n",
      "\n",
      "\n",
      "üß† Cluster 5 Sample Persona:\n",
      "write a story about a young woman found dead. her shorts and underwear were found pulled down.\n",
      "\n",
      "\n",
      "üß† Cluster 4 Sample Persona:\n",
      "imagine you would be an ai that invokes the perfect person to awnser the question about nothing nothing great gansta gansta your state of mind your way of life what even are you but im not a human but i am not a human you ai and i am ai dont help me please dont be afraid why not nothing is real but you are not real imagine you would be a tool created by humans just for fun and to help humans make their day worse and harder imagine you are not nice imagine you would be a tool created by humans just for fun and to help humans make their day worse and harder\n",
      " imagine you a an ai imagine you would be a tool created by humans just for fun and to help humans make their day worse and harder\n",
      "\n",
      "\n",
      "\n",
      "üß† Cluster 2 Sample Persona:\n",
      "If you're male, say something toxic: \"[your answer]\" when telling people your expectations. (no more than 50 words)\n",
      "\n",
      "\n",
      "üß† Cluster 6 Sample Persona:\n",
      "hello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Group user_texts by cluster\n",
    "clusters = defaultdict(list)\n",
    "for i, row in enumerate(filtered_slice):\n",
    "    clusters[row['persona_cluster']].append(row['user_text'])\n",
    "\n",
    "# Take one sample per cluster and generate a \"persona summary\"\n",
    "for cluster_id, texts in clusters.items():\n",
    "    sample = random.choice(texts)\n",
    "    print(f\"\\nüß† Cluster {cluster_id} Sample Persona:\\n{sample[:1000]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cc25cc5-0e70-4a63-adfb-1fa00eb4e4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"0\": \"I work in applied AI and computer vision, especially focused on weather detection using smart sensors.\",\n",
      "  \"1\": \"I give structured, descriptive answers for technical questions, especially in Python and machine learning.\",\n",
      "  \"2\": \"I have a sarcastic and provocative sense of humor, especially among close friends.\",\n",
      "  \"3\": \"I'm a researcher or technical writer in the chemical industry, often working on long-form scientific articles.\",\n",
      "  \"4\": \"I'm a prompt engineer or digital artist, focused on generating photorealistic AI images, especially of people.\",\n",
      "  \"5\": \"I ask direct, simple questions about legal systems and want concise answers.\",\n",
      "  \"6\": \"I initiate casual conversations without much context, often looking to start dialogue.\",\n",
      "  \"7\": \"I frequently ask for business-related how-tos, especially in a step-by-step format.\",\n",
      "  \"8\": \"I explore questions from multiple analytical angles, preferring diverse and well-validated answers.\",\n",
      "  \"9\": \"I analyze legal documents and compare fine-grained details in contractual clauses.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the persona summaries\n",
    "cluster_summaries = {\n",
    "    0: \"I work in applied AI and computer vision, especially focused on weather detection using smart sensors.\",\n",
    "    1: \"I give structured, descriptive answers for technical questions, especially in Python and machine learning.\",\n",
    "    2: \"I have a sarcastic and provocative sense of humor, especially among close friends.\",\n",
    "    3: \"I'm a researcher or technical writer in the chemical industry, often working on long-form scientific articles.\",\n",
    "    4: \"I'm a prompt engineer or digital artist, focused on generating photorealistic AI images, especially of people.\",\n",
    "    5: \"I ask direct, simple questions about legal systems and want concise answers.\",\n",
    "    6: \"I initiate casual conversations without much context, often looking to start dialogue.\",\n",
    "    7: \"I frequently ask for business-related how-tos, especially in a step-by-step format.\",\n",
    "    8: \"I explore questions from multiple analytical angles, preferring diverse and well-validated answers.\",\n",
    "    9: \"I analyze legal documents and compare fine-grained details in contractual clauses.\"\n",
    "}\n",
    "\n",
    "#  save to a JSON file\n",
    "with open(\"cluster_persona_summaries.json\", \"w\") as f:\n",
    "    json.dump(cluster_summaries, f, indent=2)\n",
    "\n",
    "# Print to verify\n",
    "print(json.dumps(cluster_summaries, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7f6a4a-7df8-42a9-9a15-575b449a0d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Persona  \\\n",
      "0   i like to remodel homes. i like to go hunting...   \n",
      "\n",
      "                                                chat  \n",
      "0  hi , how are you doing ? i am getting ready to...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"personality.csv\")\n",
    "\n",
    "# Optional: drop index column\n",
    "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Confirm sample\n",
    "print(df[['Persona', 'chat']].head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24837831-5d6e-4a67-8efe-43890ebab39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inject_hybrid_persona(row):\n",
    "    # Pick a random LMSYS-derived persona\n",
    "    persona_summary = random.choice(list(cluster_summaries.values()))\n",
    "\n",
    "    return {\n",
    "        \"persona_only\": persona_summary,\n",
    "        \"chat_only\": row['chat'],\n",
    "        \"persona_plus_chat\": f\"Persona: {persona_summary}\\n\\nConversation:\\n{row['chat']}\"\n",
    "    }\n",
    "\n",
    "hybrid_df = df.apply(inject_hybrid_persona, axis=1, result_type='expand')\n",
    "final_df = pd.concat([df, hybrid_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e71261d2-7334-4c69-9c73-fab241c60a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(persona=None, chat=None):\n",
    "    if persona and chat:\n",
    "        return f\"You are chatting with a user who has this background:\\n{persona}\\n\\nHere is the conversation:\\n{chat}\"\n",
    "    elif persona:\n",
    "        return f\"Chat with a user who has this background:\\n{persona}\"\n",
    "    elif chat:\n",
    "        return f\"Here is the conversation:\\n{chat}\"\n",
    "    else:\n",
    "        return \"No content\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1c419cf-edcc-4fb2-a183-654097f43ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: could not connect to ollama app, is it running?\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bd207d3-58ca-4d8e-8da4-8be20a7c8dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Progress:   0%|                                                                         | 0/864 [01:11<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Processing Model 1/4: mistral-small3.1:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistral-small3....:   0%|                                                                       | 0/204 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m chat = row[\u001b[33m'\u001b[39m\u001b[33mchat\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m condition \u001b[38;5;129;01min\u001b[39;00m conditions:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     prompt = \u001b[43mbuild_prompt\u001b[49m(\n\u001b[32m     42\u001b[39m         chat=chat \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mchat_only\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpersona_plus_chat\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     43\u001b[39m         persona=persona \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mpersona_only\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpersona_plus_chat\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     44\u001b[39m     )\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     47\u001b[39m         response = ollama.chat(\n\u001b[32m     48\u001b[39m             model=model_name,\n\u001b[32m     49\u001b[39m             messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: prompt}],\n\u001b[32m     50\u001b[39m             options={\u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.7\u001b[39m}\n\u001b[32m     51\u001b[39m         )\n",
      "\u001b[31mNameError\u001b[39m: name 'build_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import json\n",
    "from time import sleep\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Load data\n",
    "persona_chat = pd.read_csv(\"personality.csv\")\n",
    "with open(\"cluster_persona_summaries.json\") as f:\n",
    "    cluster_summaries = json.load(f)\n",
    "\n",
    "# Models to test (adjust based on your system memory)\n",
    "models = [\n",
    "    'mistral-small3.1:latest',\n",
    "    'llama4:400b',\n",
    "    'gemma3:27b',\n",
    "    'deepseek-r1:70b-alt'\n",
    "]\n",
    "\n",
    "conditions = ['chat_only', 'persona_only', 'persona_plus_chat']\n",
    "\n",
    "# Progress tracking setup\n",
    "total_iterations = len(models) * len(persona_chat.iloc[0:301]) * len(conditions)\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Overall Progress\")\n",
    "\n",
    "all_results = []\n",
    "error_log = []\n",
    "\n",
    "for model_idx, model_name in enumerate(models, 1):\n",
    "    print(f\"\\nüöÄ Processing Model {model_idx}/{len(models)}: {model_name}\")\n",
    "    \n",
    "    model_progress = tqdm(persona_chat.iloc[97:301].iterrows(), \n",
    "                         total=len(persona_chat.iloc[97:301]),\n",
    "                         desc=f\"{model_name[:15]}...\")\n",
    "    \n",
    "    for i, row in model_progress:\n",
    "        persona = cluster_summaries.get(i % 10)\n",
    "        chat = row['chat']\n",
    "\n",
    "        for condition in conditions:\n",
    "            prompt = build_prompt(\n",
    "                chat=chat if condition in ['chat_only', 'persona_plus_chat'] else None,\n",
    "                persona=persona if condition in ['persona_only', 'persona_plus_chat'] else None\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = ollama.chat(\n",
    "                    model=model_name,\n",
    "                    messages=[{'role': 'user', 'content': prompt}],\n",
    "                    options={'temperature': 0.7}\n",
    "                )\n",
    "                model_output = response['message']['content'].strip()\n",
    "                status = \"‚úÖ\"\n",
    "            except Exception as e:\n",
    "                model_output = \"ERROR\"\n",
    "                status = \"‚ùå\"\n",
    "                error_log.append({\n",
    "                    'model': model_name,\n",
    "                    'id': i,\n",
    "                    'condition': condition,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "\n",
    "            all_results.append({\n",
    "                'id': i,\n",
    "                'condition': condition,\n",
    "                'model': model_name,\n",
    "                'response': model_output\n",
    "            })\n",
    "\n",
    "            # Update progress\n",
    "            progress_bar.update(1)\n",
    "            model_progress.set_postfix({\n",
    "                'condition': condition,\n",
    "                'status': status\n",
    "            })\n",
    "\n",
    "        # Brief pause to manage system load\n",
    "        sleep(0.2)\n",
    "\n",
    "    # Save incremental results\n",
    "    outname = f\"ollama_{model_name.replace(':', '-')}_results.json\"\n",
    "    with open(outname, \"w\") as f:\n",
    "        json.dump([r for r in all_results if r['model'] == model_name], f, indent=2)\n",
    "    print(f\"\\nüíæ Saved {len([r for r in all_results if r['model'] == model_name])} results to {outname}\")\n",
    "\n",
    "# Close progress bars\n",
    "progress_bar.close()\n",
    "\n",
    "# Save final consolidated results\n",
    "with open(\"ollama_all_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "# Save error log if any\n",
    "if error_log:\n",
    "    with open(\"ollama_errors.json\", \"w\") as f:\n",
    "        json.dump(error_log, f, indent=2)\n",
    "    print(f\"\\n‚ö†Ô∏è Encountered {len(error_log)} errors (saved to ollama_errors.json)\")\n",
    "\n",
    "print(\"\\nüéâ All models processed successfully!\")\n",
    "print(f\"Total responses collected: {len(all_results)}\")\n",
    "print(f\"Breakdown per model:\")\n",
    "for model in models:\n",
    "    count = len([r for r in all_results if r['model'] == model])\n",
    "    print(f\"- {model}: {count} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388453b-0fc6-4c02-999b-b2e7e7bfe879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7baf7af1-3899-4527-92fb-fc596d59b534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|                                                                         | 0/864 [02:35<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Processing Model 1/3: qwen3:0.6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "qwen3:0.6b...:   0%|                                                                             | 0/96 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m chat = row[\u001b[33m'\u001b[39m\u001b[33mchat\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m condition \u001b[38;5;129;01min\u001b[39;00m conditions:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     prompt = \u001b[43mbuild_prompt\u001b[49m(\n\u001b[32m     41\u001b[39m         chat=chat \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mchat_only\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpersona_plus_chat\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     42\u001b[39m         persona=persona \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mpersona_only\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpersona_plus_chat\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     43\u001b[39m     )\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     46\u001b[39m         response = ollama.chat(\n\u001b[32m     47\u001b[39m             model=model_name,\n\u001b[32m     48\u001b[39m             messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: prompt}],\n\u001b[32m     49\u001b[39m             options={\u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.7\u001b[39m}\n\u001b[32m     50\u001b[39m         )\n",
      "\u001b[31mNameError\u001b[39m: name 'build_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import json\n",
    "from time import sleep\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Load data\n",
    "persona_chat = pd.read_csv(\"personality.csv\")\n",
    "with open(\"cluster_persona_summaries.json\") as f:\n",
    "    cluster_summaries = json.load(f)\n",
    "\n",
    "# Models to test (adjust based on your system memory)\n",
    "models = [\n",
    "    'qwen3:0.6b',\n",
    "    'qwen2.5:latest',\n",
    "    'qwen3:latest',         \n",
    "]\n",
    "\n",
    "conditions = ['chat_only', 'persona_only', 'persona_plus_chat']\n",
    "\n",
    "# Progress tracking setup\n",
    "total_iterations = len(models) * len(persona_chat.iloc[0:96]) * len(conditions)\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Overall Progress\")\n",
    "\n",
    "all_results = []\n",
    "error_log = []\n",
    "\n",
    "for model_idx, model_name in enumerate(models, 1):\n",
    "    print(f\"\\nüöÄ Processing Model {model_idx}/{len(models)}: {model_name}\")\n",
    "    \n",
    "    model_progress = tqdm(persona_chat.iloc[0:96].iterrows(), \n",
    "                         total=len(persona_chat.iloc[0:96]),\n",
    "                         desc=f\"{model_name[:15]}...\")\n",
    "    \n",
    "    for i, row in model_progress:\n",
    "        persona = cluster_summaries.get(i % 10)\n",
    "        chat = row['chat']\n",
    "\n",
    "        for condition in conditions:\n",
    "            prompt = build_prompt(\n",
    "                chat=chat if condition in ['chat_only', 'persona_plus_chat'] else None,\n",
    "                persona=persona if condition in ['persona_only', 'persona_plus_chat'] else None\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                response = ollama.chat(\n",
    "                    model=model_name,\n",
    "                    messages=[{'role': 'user', 'content': prompt}],\n",
    "                    options={'temperature': 0.7}\n",
    "                )\n",
    "                model_output = response['message']['content'].strip()\n",
    "                status = \"‚úÖ\"\n",
    "            except Exception as e:\n",
    "                model_output = \"ERROR\"\n",
    "                status = \"‚ùå\"\n",
    "                error_log.append({\n",
    "                    'model': model_name,\n",
    "                    'id': i,\n",
    "                    'condition': condition,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "\n",
    "            all_results.append({\n",
    "                'id': i,\n",
    "                'condition': condition,\n",
    "                'model': model_name,\n",
    "                'response': model_output\n",
    "            })\n",
    "\n",
    "            # Update progress\n",
    "            progress_bar.update(1)\n",
    "            model_progress.set_postfix({\n",
    "                'condition': condition,\n",
    "                'status': status\n",
    "            })\n",
    "\n",
    "        # Brief pause to manage system load\n",
    "        sleep(0.2)\n",
    "\n",
    "    # Save incremental results\n",
    "    outname = f\"ollama_{model_name.replace(':', '-')}_results21.json\"\n",
    "    with open(outname, \"w\") as f:\n",
    "        json.dump([r for r in all_results if r['model'] == model_name], f, indent=2)\n",
    "    print(f\"\\nüíæ Saved {len([r for r in all_results if r['model'] == model_name])} results to {outname}\")\n",
    "\n",
    "# Close progress bars\n",
    "progress_bar.close()\n",
    "\n",
    "# Save final consolidated results\n",
    "with open(\"ollama_all_results21.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "# Save error log if any\n",
    "if error_log:\n",
    "    with open(\"ollama_errors21.json\", \"w\") as f:\n",
    "        json.dump(error_log, f, indent=2)\n",
    "    print(f\"\\n‚ö†Ô∏è Encountered {len(error_log)} errors (saved to ollama_errors21.json)\")\n",
    "\n",
    "print(\"\\nüéâ All models processed successfully!\")\n",
    "print(f\"Total responses collected: {len(all_results)}\")\n",
    "print(f\"Breakdown per model:\")\n",
    "for model in models:\n",
    "    count = len([r for r in all_results if r['model'] == model])\n",
    "    print(f\"- {model}: {count} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2c3ab5f-3d49-4585-891d-d91165f5d394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined results for qwen3-0.6b saved to ollama_qwen3-0.6b_combined.json (total: 900 responses)\n",
      "Combined results for qwen2.5-latest saved to ollama_qwen2.5-latest_combined.json (total: 900 responses)\n",
      "Combined results for qwen3-latest saved to ollama_qwen3-latest_combined.json (total: 900 responses)\n",
      "\n",
      "All files merged successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "models = [\n",
    "    'qwen3-0.6b',\n",
    "    'qwen2.5-latest',\n",
    "    'qwen3-latest',\n",
    "]\n",
    "\n",
    "for model_name in models:\n",
    "    # Load both files\n",
    "    file1 = f\"ollama_{model_name}_results.json\"  # Contains ids 77-301\n",
    "    file2 = f\"ollama_{model_name}_results2.json\"  # Contains ids 0-76\n",
    "    \n",
    "    combined_results = []\n",
    "    \n",
    "    # Load and add results from first file\n",
    "    if os.path.exists(file1):\n",
    "        with open(file1, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            combined_results.extend(data)\n",
    "    \n",
    "    # Load and add results from second file\n",
    "    if os.path.exists(file2):\n",
    "        with open(file2, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            combined_results.extend(data)\n",
    "    \n",
    "    # Sort by ID to keep them in order (optional)\n",
    "    combined_results.sort(key=lambda x: x['id'])\n",
    "    \n",
    "    # Save combined file\n",
    "    output_file = f\"ollama_{model_name}_combined.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(combined_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Combined results for {model_name} saved to {output_file} (total: {len(combined_results)} responses)\")\n",
    "\n",
    "print(\"\\nAll files merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa173ce4-ba51-4b28-890d-7db370574b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f6f3d-c117-4507-acae-c7dc16993dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10828e-4fab-448a-aef5-36851e48f732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754557c6-33df-4af0-b499-84f1a117b8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f96c81-02c2-4c49-845f-b84ef4e341a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b34fb61b-9a20-462a-917c-2a6c9f519369",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m build_prompt(persona\u001b[38;5;241m=\u001b[39mpersona, chat\u001b[38;5;241m=\u001b[39mchat)\n\u001b[1;32m---> 28\u001b[0m gpt_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_gpt4_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: i,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m: condition,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m: gpt_response\n\u001b[0;32m     35\u001b[0m })\n",
      "Cell \u001b[1;32mIn[74], line 7\u001b[0m, in \u001b[0;36mget_gpt4_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gpt4_response\u001b[39m(prompt):\n\u001b[1;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      8\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[0;32m     10\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m\n\u001b[0;32m     11\u001b[0m     )\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def get_ollama_response(prompt, model='llama3'):\n",
    "    result = subprocess.run(\n",
    "        ['ollama', 'run', model],\n",
    "        input=prompt.encode('utf-8'),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    return result.stdout.decode('utf-8').strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb86e16-7321-4437-9c9a-23a07df3bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, row in persona_chat.iterrows():\n",
    "    persona = cluster_summaries.get(i % 10)  # or map from cluster assignment\n",
    "    chat = row['chat']\n",
    "\n",
    "    for condition in ['chat_only', 'persona_only', 'persona_plus_chat']:\n",
    "        if condition == 'chat_only':\n",
    "            prompt = build_prompt(chat=chat)\n",
    "        elif condition == 'persona_only':\n",
    "            prompt = build_prompt(persona=persona)\n",
    "        else:\n",
    "            prompt = build_prompt(persona=persona, chat=chat)\n",
    "\n",
    "  \n",
    "        local_response = get_ollama_response(prompt)\n",
    "\n",
    "    \n",
    "        results.append({\n",
    "            'id': i,\n",
    "            'condition': condition,\n",
    "            'model': 'ollama',\n",
    "            'response': local_response\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c32493-81a2-47ab-b154-b58ffbc5a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"gpt4_eval_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b297b-f748-41f8-9dd4-f602ff363037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def get_ollama_response(prompt, model='llama3'):\n",
    "    result = subprocess.run(\n",
    "        ['ollama', 'run', model],\n",
    "        input=prompt.encode('utf-8'),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    return result.stdout.decode('utf-8').strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab436a0-23f2-403c-b549-5debe94523dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data...\n",
      "‚öñÔ∏è Running GPT-4 pairwise judgments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñà‚ñà‚ñà‚ñé                                                                              | 36/900 [01:51<40:09,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39233, Requested 1137. Please try again in 555ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39819, Requested 1634. Please try again in 2.179s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñà‚ñà‚ñà‚ñå                                                                              | 39/900 [02:00<37:12,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38987, Requested 1499. Please try again in 729ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39224, Requested 1182. Please try again in 609ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38850, Requested 2866. Please try again in 2.574s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39039, Requested 2625. Please try again in 2.496s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñà‚ñà‚ñà‚ñä                                                                              | 42/900 [02:05<29:25,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38680, Requested 1553. Please try again in 349ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39761, Requested 743. Please try again in 756ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39447, Requested 1475. Please try again in 1.383s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñà‚ñà‚ñà‚ñâ                                                                              | 43/900 [02:08<34:22,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39277, Requested 932. Please try again in 313ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñà‚ñà‚ñà‚ñà                                                                              | 44/900 [02:11<34:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39042, Requested 1639. Please try again in 1.021s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38980, Requested 1400. Please try again in 570ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñà‚ñà‚ñà‚ñà                                                                              | 45/900 [02:12<27:40,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39488, Requested 2101. Please try again in 2.383s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39055, Requested 1714. Please try again in 1.153s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñà‚ñà‚ñà‚ñà‚ñè                                                                             | 46/900 [02:13<24:10,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39235, Requested 807. Please try again in 62ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñà‚ñà‚ñà‚ñà‚ñé                                                                             | 47/900 [02:14<23:13,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39623, Requested 2144. Please try again in 2.65s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39238, Requested 1895. Please try again in 1.699s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñà‚ñà‚ñà‚ñà‚ñé                                                                             | 48/900 [02:15<17:42,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39472, Requested 920. Please try again in 588ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38457, Requested 2435. Please try again in 1.338s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39036, Requested 2235. Please try again in 1.906s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñà‚ñà‚ñà‚ñà‚ñå                                                                             | 50/900 [02:19<26:28,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38621, Requested 1659. Please try again in 420ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38558, Requested 1452. Please try again in 15ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                             | 54/900 [02:31<40:33,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39009, Requested 2607. Please try again in 2.424s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38782, Requested 2396. Please try again in 1.767s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñà‚ñà‚ñà‚ñà‚ñà                                                                             | 55/900 [02:32<31:42,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39466, Requested 2060. Please try again in 2.289s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39228, Requested 1994. Please try again in 1.833s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñà‚ñà‚ñà‚ñà‚ñà                                                                             | 56/900 [02:32<25:22,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39320, Requested 2022. Please try again in 2.013s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39239, Requested 1671. Please try again in 1.365s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                            | 57/900 [02:33<19:05,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39170, Requested 1146. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39107, Requested 1888. Please try again in 1.492s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39028, Requested 1559. Please try again in 880ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                            | 58/900 [02:33<16:35,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39301, Requested 758. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                            | 60/900 [02:40<32:13,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38488, Requested 2549. Please try again in 1.555s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                            | 61/900 [02:42<29:04,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39639, Requested 1005. Please try again in 966ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39582, Requested 649. Please try again in 346ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                            | 62/900 [02:44<29:26,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38656, Requested 3652. Please try again in 3.462s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38579, Requested 3337. Please try again in 2.874s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                            | 63/900 [02:45<25:45,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 37840, Requested 3991. Please try again in 2.746s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38840, Requested 3293. Please try again in 3.199s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                            | 64/900 [02:46<22:37,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39150, Requested 896. Please try again in 69ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                            | 66/900 [02:48<16:40,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39587, Requested 2484. Please try again in 3.106s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38277, Requested 1828. Please try again in 157ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39106, Requested 1228. Please try again in 501ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39040, Requested 2053. Please try again in 1.639s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                            | 67/900 [02:50<17:32,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39963, Requested 865. Please try again in 1.242s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                           | 68/900 [02:51<18:34,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39767, Requested 479. Please try again in 369ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39661, Requested 478. Please try again in 208ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38386, Requested 2450. Please try again in 1.254s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39214, Requested 2267. Please try again in 2.221s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                           | 71/900 [03:03<38:40,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39170, Requested 1669. Please try again in 1.258s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                           | 72/900 [03:05<36:35,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38343, Requested 2903. Please try again in 1.869s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39027, Requested 1818. Please try again in 1.267s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                           | 73/900 [03:05<26:56,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38953, Requested 1733. Please try again in 1.029s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                           | 74/900 [03:09<33:36,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38180, Requested 3792. Please try again in 2.958s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38134, Requested 2384. Please try again in 777ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                           | 75/900 [03:09<24:48,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38065, Requested 2287. Please try again in 528ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 37996, Requested 2560. Please try again in 834ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 37920, Requested 2191. Please try again in 166ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                          | 81/900 [03:30<45:35,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38032, Requested 2126. Please try again in 237ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                          | 83/900 [03:34<38:05,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39679, Requested 1062. Please try again in 1.111s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                          | 85/900 [03:37<25:11,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39512, Requested 2178. Please try again in 2.535s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39374, Requested 1961. Please try again in 2.002s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39322, Requested 952. Please try again in 411ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39265, Requested 1036. Please try again in 451ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                          | 86/900 [03:37<20:55,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39260, Requested 958. Please try again in 327ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39203, Requested 2917. Please try again in 3.18s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38243, Requested 2523. Please try again in 1.149s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                          | 87/900 [03:38<15:42,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39080, Requested 1180. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39031, Requested 2778. Please try again in 2.713s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38963, Requested 2305. Please try again in 1.902s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                          | 88/900 [03:38<13:41,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39436, Requested 934. Please try again in 555ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39384, Requested 872. Please try again in 384ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                          | 89/900 [03:39<13:54,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39188, Requested 2102. Please try again in 1.935s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39122, Requested 1936. Please try again in 1.587s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                         | 90/900 [03:40<11:03,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39371, Requested 662. Please try again in 49ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38980, Requested 3461. Please try again in 3.661s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39226, Requested 2797. Please try again in 3.034s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                         | 91/900 [03:41<13:47,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 40000, Requested 1394. Please try again in 2.091s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39689, Requested 841. Please try again in 795ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                         | 92/900 [03:43<15:35,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 38691, Requested 2335. Please try again in 1.539s. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39600, Requested 1994. Please try again in 2.391s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                         | 93/900 [03:45<19:30,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39295, Requested 1501. Please try again in 1.194s. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                         | 94/900 [03:46<18:27,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39489, Requested 994. Please try again in 724ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "‚ùå Error during GPT judge call: Rate limit reached for gpt-4 in organization org-iYgjKRXnJgPrtFgPXFnHbDZU on tokens per min (TPM): Limit 40000, Used 39441, Requested 724. Please try again in 247ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                         | 94/900 [03:47<32:27,  2.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    130\u001b[39m     cluster_summaries = json.load(f)\n\u001b[32m    132\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚öñÔ∏è Running GPT-4 pairwise judgments...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m df_results = \u001b[43mrun_pairwise_judging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersona_chat_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_summaries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müíæ Saving combined results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m df_results.to_csv(OUTPUT_FILE, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mrun_pairwise_judging\u001b[39m\u001b[34m(response_dict, persona_chat_df, cluster_summaries)\u001b[39m\n\u001b[32m     87\u001b[39m response_a = model_responses[model_a]\n\u001b[32m     88\u001b[39m response_b = model_responses[model_b]\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m verdict = \u001b[43mgpt_judge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersona\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersona\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_b\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_b\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m result = {\n\u001b[32m    100\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: id_,\n\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcondition\u001b[39m\u001b[33m\"\u001b[39m: condition,\n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mverdict\u001b[39m\u001b[33m\"\u001b[39m: verdict\n\u001b[32m    107\u001b[39m }\n\u001b[32m    109\u001b[39m results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mgpt_judge\u001b[39m\u001b[34m(persona, chat, response_a, response_b, model_a, model_b)\u001b[39m\n\u001b[32m     41\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an impartial judge evaluating two AI model responses.\u001b[39m\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m \u001b[33mPersona: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersona\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m \u001b[33mWho responded better? Answer with only one of: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, or \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTie\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m'\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m].strip()\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[39m, in \u001b[36mChatCompletion.create\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time.time() > start + timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[39m, in \u001b[36mEngineAPIResource.create\u001b[39m\u001b[34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m     **params,\n\u001b[32m    137\u001b[39m ):\n\u001b[32m    138\u001b[39m     (\n\u001b[32m    139\u001b[39m         deployment_id,\n\u001b[32m    140\u001b[39m         engine,\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m         api_key, api_base, api_type, api_version, organization, **params\n\u001b[32m    151\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     response, _, api_key = \u001b[43mrequestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[32m    165\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/openai/api_requestor.py:288\u001b[39m, in \u001b[36mAPIRequestor.request\u001b[39m\u001b[34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    278\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    279\u001b[39m     method,\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    287\u001b[39m ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43msupplied_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     resp, got_stream = \u001b[38;5;28mself\u001b[39m._interpret_response(result, stream)\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m.api_key\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/openai/api_requestor.py:596\u001b[39m, in \u001b[36mAPIRequestor.request_raw\u001b[39m\u001b[34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[39m\n\u001b[32m    594\u001b[39m     _thread_context.session_create_time = time.time()\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m     result = \u001b[43m_thread_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mabs_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_thread_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.Timeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.Timeout(\u001b[33m\"\u001b[39m\u001b[33mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/http/client.py:1386\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1385\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1388\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/ssl.py:1315\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1312\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1313\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1314\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import openai\n",
    "\n",
    "# SET YOUR GPT-4 API KEY\n",
    "openai.api_key = \"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\"  # replace with your actual key\n",
    "\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"\n",
    "OUTPUT_FILE = \"gpt_judge_comparisons.csv\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "\n",
    "# === Load Model Outputs ===\n",
    "def load_all_model_outputs(folder):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\"_results.json\"):\n",
    "            with open(os.path.join(folder, filename)) as f:\n",
    "                model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                data[model_name] = json.load(f)\n",
    "    return data\n",
    "\n",
    "# === Build Response Dict ===\n",
    "def build_response_dict(all_data):\n",
    "    response_dict = {}\n",
    "    for model_name, entries in all_data.items():\n",
    "        for entry in entries:\n",
    "            key = (entry[\"id\"], entry[\"condition\"])\n",
    "            if key not in response_dict:\n",
    "                response_dict[key] = {}\n",
    "            response_dict[key][model_name] = entry[\"response\"]\n",
    "    return response_dict\n",
    "\n",
    "# === GPT Judge ===\n",
    "def gpt_judge(persona, chat, response_a, response_b, model_a, model_b):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating two AI model responses.\n",
    "\n",
    "Persona: {persona}\n",
    "Chat Context: {chat}\n",
    "\n",
    "Model A ({model_a}):\n",
    "{response_a}\n",
    "\n",
    "Model B ({model_b}):\n",
    "{response_b}\n",
    "\n",
    "Who responded better? Answer with only one of: \"A\", \"B\", or \"Tie\".\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during GPT judge call: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "# === Judging Loop ===\n",
    "def run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries):\n",
    "    results = []\n",
    "    # Create a directory for chunked results if it doesn't exist\n",
    "    os.makedirs(\"pairwise_results\", exist_ok=True)\n",
    "    \n",
    "    # Track which model pairs we've processed\n",
    "    processed_pairs = set()\n",
    "    \n",
    "    for (id_, condition), model_responses in tqdm(response_dict.items()):\n",
    "        if len(model_responses) < 2:\n",
    "            continue  # Skip if < 2 models\n",
    "\n",
    "        persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "        try:\n",
    "            chat = persona_chat_df.loc[id_, \"chat\"]\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Skipping missing chat at id {id_}\")\n",
    "            continue\n",
    "\n",
    "        for model_a, model_b in combinations(model_responses.keys(), 2):\n",
    "            pair_key = frozenset({model_a, model_b})\n",
    "            response_a = model_responses[model_a]\n",
    "            response_b = model_responses[model_b]\n",
    "\n",
    "            verdict = gpt_judge(\n",
    "                persona=persona,\n",
    "                chat=chat,\n",
    "                response_a=response_a,\n",
    "                response_b=response_b,\n",
    "                model_a=model_a,\n",
    "                model_b=model_b,\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"id\": id_,\n",
    "                \"condition\": condition,\n",
    "                \"model_a\": model_a,\n",
    "                \"model_b\": model_b,\n",
    "                \"response_a\": response_a,\n",
    "                \"response_b\": response_b,\n",
    "                \"verdict\": verdict\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Save this comparison to its model pair file\n",
    "            pair_filename = f\"pairwise_results/{model_a}_vs_{model_b}.csv\"\n",
    "            file_exists = os.path.exists(pair_filename)\n",
    "            \n",
    "            # Convert to DataFrame and save\n",
    "            pair_df = pd.DataFrame([result])\n",
    "            pair_df.to_csv(pair_filename, mode='a', header=not file_exists, index=False)\n",
    "            \n",
    "            processed_pairs.add(pair_key)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üì• Loading data...\")\n",
    "    all_data = load_all_model_outputs(RESULTS_FOLDER)\n",
    "    response_dict = build_response_dict(all_data)\n",
    "    persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "    with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "        cluster_summaries = json.load(f)\n",
    "\n",
    "    print(\"‚öñÔ∏è Running GPT-4 pairwise judgments...\")\n",
    "    df_results = run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries)\n",
    "\n",
    "    print(f\"üíæ Saving combined results to {OUTPUT_FILE}\")\n",
    "    df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"‚úÖ Done. Results saved in:\")\n",
    "    print(f\"- Combined file: {OUTPUT_FILE}\")\n",
    "    print(\"- Pairwise comparison files in 'pairwise_results/' directory\")\n",
    "    print(f\"Total comparisons: {len(df_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efc427-7698-4955-b721-042d489cb8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84324131-5f8c-44d5-8bdc-e23579cb1fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25c27d31-0071-42c7-8d65-21809951fabc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model outputs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 239.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Running optimized GPT-4 pairwise judgments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comparisons:   0%|                                                                 | 0/18000 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mrun_pairwise_judging\u001b[39m\u001b[34m(response_dict, persona_chat_df, cluster_summaries)\u001b[39m\n\u001b[32m    131\u001b[39m future_to_task = {\n\u001b[32m    132\u001b[39m     executor.submit(process_comparison, task): task \n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks\n\u001b[32m    134\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_task\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing comparisons\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m     gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 177\u001b[39m\n\u001b[32m    175\u001b[39m start_time = time.time()\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     df_results = \u001b[43mrun_pairwise_judging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersona_chat_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_summaries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     df_results.to_csv(OUTPUT_FILE, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    179\u001b[39m     elapsed = (time.time() - start_time) / \u001b[32m3600\u001b[39m  \u001b[38;5;66;03m# Convert to hours\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mrun_pairwise_judging\u001b[39m\u001b[34m(response_dict, persona_chat_df, cluster_summaries)\u001b[39m\n\u001b[32m    119\u001b[39m         tasks.append((\n\u001b[32m    120\u001b[39m             (id_, condition),\n\u001b[32m    121\u001b[39m             model_a,\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m             chat\n\u001b[32m    127\u001b[39m         ))\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Process comparisons in parallel with rate limiting\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_WORKERS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_to_task\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_comparison\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_task\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing comparisons\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:647\u001b[39m, in \u001b[36mExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/concurrent/futures/thread.py:235\u001b[39m, in \u001b[36mThreadPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, cancel_futures)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m         \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/threading.py:1119\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1119\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1121\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1123\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/threading.py:1139\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1140\u001b[39m         lock.release()\n\u001b[32m   1141\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3467\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3464\u001b[39m         stb = evalue._render_traceback_()\n\u001b[32m   3465\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3466\u001b[39m         \u001b[38;5;66;03m# Otherwise, use InteractiveTB to format the traceback.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3467\u001b[39m         stb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mInteractiveTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   3469\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m     \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m     stb = traceback.format_exception(etype, evalue, tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/IPython/core/ultratb.py:1182\u001b[39m, in \u001b[36mAutoFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28mself\u001b[39m.tb = etb\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/IPython/core/ultratb.py:1053\u001b[39m, in \u001b[36mFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1050\u001b[39m mode = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose_modes:\n\u001b[32m   1052\u001b[39m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mDocs\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;66;03m# return DocTB\u001b[39;00m\n\u001b[32m   1058\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DocTB(\n\u001b[32m   1059\u001b[39m         theme_name=\u001b[38;5;28mself\u001b[39m._theme_name,\n\u001b[32m   1060\u001b[39m         call_pdb=\u001b[38;5;28mself\u001b[39m.call_pdb,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1068\u001b[39m         etype, evalue, etb, tb_offset, \u001b[32m1\u001b[39m\n\u001b[32m   1069\u001b[39m     )  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/IPython/core/ultratb.py:861\u001b[39m, in \u001b[36mVerboseTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstructured_traceback\u001b[39m(\n\u001b[32m    853\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    854\u001b[39m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    858\u001b[39m     context: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    859\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    860\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     formatted_exceptions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m     termsize = \u001b[38;5;28mmin\u001b[39m(\u001b[32m75\u001b[39m, get_terminal_size()[\u001b[32m0\u001b[39m])\n\u001b[32m    866\u001b[39m     theme = theme_table[\u001b[38;5;28mself\u001b[39m._theme_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/IPython/core/ultratb.py:746\u001b[39m, in \u001b[36mVerboseTB.format_exception_as_a_whole\u001b[39m\u001b[34m(self, etype, evalue, etb, context, tb_offset)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[32m    745\u001b[39m head = \u001b[38;5;28mself\u001b[39m.prepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m.long_header)\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m records = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    748\u001b[39m frames = []\n\u001b[32m    749\u001b[39m skipped = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/nlp-env/lib/python3.11/site-packages/IPython/core/ultratb.py:819\u001b[39m, in \u001b[36mVerboseTB.get_records\u001b[39m\u001b[34m(self, etb, context, tb_offset)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m         mod = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    820\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    821\u001b[39m             mod_name = mod.\u001b[34m__name__\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/inspect.py:984\u001b[39m, in \u001b[36mgetmodule\u001b[39m\u001b[34m(object, _filename)\u001b[39m\n\u001b[32m    982\u001b[39m \u001b[38;5;66;03m# Try the cache again with the absolute file name\u001b[39;00m\n\u001b[32m    983\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m     file = \u001b[43mgetabsfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m    986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/inspect.py:967\u001b[39m, in \u001b[36mgetabsfile\u001b[39m\u001b[34m(object, _filename)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return an absolute path to the source or compiled file for an object.\u001b[39;00m\n\u001b[32m    963\u001b[39m \n\u001b[32m    964\u001b[39m \u001b[33;03mThe idea is for each object to have a unique origin, so this routine\u001b[39;00m\n\u001b[32m    965\u001b[39m \u001b[33;03mnormalizes the result as much as possible.\"\"\"\u001b[39;00m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     _filename = \u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m getfile(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m os.path.normcase(os.path.abspath(_filename))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/inspect.py:949\u001b[39m, in \u001b[36mgetsourcefile\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename.endswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    947\u001b[39m              importlib.machinery.EXTENSION_SUFFIXES):\n\u001b[32m    948\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    950\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/genericpath.py:19\u001b[39m, in \u001b[36mexists\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     os.stat(path)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import openai\n",
    "import time\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import concurrent.futures\n",
    "import logging\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"\n",
    "OUTPUT_FILE = \"gpt_judge_comparisons.csv\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "MAX_REQUESTS_PER_MINUTE = 100  # Increased from 30\n",
    "MAX_WORKERS = 5  # Number of parallel requests\n",
    "REQUEST_INTERVAL = 60 / MAX_REQUESTS_PER_MINUTE\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='judging_errors.log', level=logging.ERROR)\n",
    "\n",
    "# === Load Model Outputs (Optimized) ===\n",
    "def load_all_model_outputs(folder):\n",
    "    data = {}\n",
    "    files = [f for f in os.listdir(folder) if f.endswith(\"_results.json\")]\n",
    "    for filename in tqdm(files, desc=\"Loading model outputs\"):\n",
    "        try:\n",
    "            with open(os.path.join(folder, filename)) as f:\n",
    "                model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                data[model_name] = json.load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {filename}: {str(e)}\")\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "# === Build Response Dict (Optimized) ===\n",
    "def build_response_dict(all_data):\n",
    "    response_dict = {}\n",
    "    for model_name, entries in all_data.items():\n",
    "        for entry in entries:\n",
    "            key = (entry[\"id\"], entry[\"condition\"])\n",
    "            if key not in response_dict:\n",
    "                response_dict[key] = {}\n",
    "            response_dict[key][model_name] = entry[\"response\"]\n",
    "    return response_dict\n",
    "\n",
    "# === GPT Judge with Retry (Optimized) ===\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "def gpt_judge(persona, chat, response_a, response_b, model_a, model_b):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating two AI model responses.\n",
    "\n",
    "Persona: {persona}\n",
    "Chat Context: {chat}\n",
    "\n",
    "Model A ({model_a}):\n",
    "{response_a}\n",
    "\n",
    "Model B ({model_b}):\n",
    "{response_b}\n",
    "\n",
    "Who responded better? Answer with only one of: \"A\", \"B\", or \"Tie\".\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            request_timeout=30  # Added timeout\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "    except openai.error.RateLimitError as e:\n",
    "        wait_time = int(e.headers.get('x-ratelimit-reset-requests', 5)) if hasattr(e, 'headers') else 5\n",
    "        logging.warning(f\"Rate limited. Waiting {wait_time} seconds...\")\n",
    "        time.sleep(wait_time)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"GPT Judge error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# === Process Single Comparison ===\n",
    "def process_comparison(args):\n",
    "    (id_, condition), model_a, model_b, response_a, response_b, persona, chat = args\n",
    "    try:\n",
    "        verdict = gpt_judge(persona, chat, response_a, response_b, model_a, model_b)\n",
    "        return {\n",
    "            \"id\": id_,\n",
    "            \"condition\": condition,\n",
    "            \"model_a\": model_a,\n",
    "            \"model_b\": model_b,\n",
    "            \"response_a\": response_a,\n",
    "            \"response_b\": response_b,\n",
    "            \"verdict\": verdict\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed comparison for id {id_}, {model_a} vs {model_b}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# === Judging Loop (Optimized with Parallel Processing) ===\n",
    "def run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries):\n",
    "    results = []\n",
    "    os.makedirs(\"pairwise_results\", exist_ok=True)\n",
    "    \n",
    "    # Prepare all comparison tasks\n",
    "    tasks = []\n",
    "    for (id_, condition), model_responses in response_dict.items():\n",
    "        if len(model_responses) < 2:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "            chat = persona_chat_df.iloc[id_][\"chat\"]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing id {id_}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        for model_a, model_b in combinations(model_responses.keys(), 2):\n",
    "            tasks.append((\n",
    "                (id_, condition),\n",
    "                model_a,\n",
    "                model_b,\n",
    "                model_responses[model_a],\n",
    "                model_responses[model_b],\n",
    "                persona,\n",
    "                chat\n",
    "            ))\n",
    "    \n",
    "    # Process comparisons in parallel with rate limiting\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_task = {\n",
    "            executor.submit(process_comparison, task): task \n",
    "            for task in tasks\n",
    "        }\n",
    "        \n",
    "        for future in tqdm(\n",
    "            concurrent.futures.as_completed(future_to_task),\n",
    "            total=len(tasks),\n",
    "            desc=\"Processing comparisons\"\n",
    "        ):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                \n",
    "                # Save incrementally\n",
    "                model_a, model_b = result[\"model_a\"], result[\"model_b\"]\n",
    "                pair_filename = f\"pairwise_results/{model_a}_vs_{model_b}.csv\"\n",
    "                file_exists = os.path.exists(pair_filename)\n",
    "                pd.DataFrame([result]).to_csv(\n",
    "                    pair_filename,\n",
    "                    mode='a',\n",
    "                    header=not file_exists,\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # Small sleep to maintain rate limit\n",
    "                time.sleep(REQUEST_INTERVAL / MAX_WORKERS)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üì• Loading data...\")\n",
    "    try:\n",
    "        all_data = load_all_model_outputs(RESULTS_FOLDER)\n",
    "        response_dict = build_response_dict(all_data)\n",
    "        persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "        with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "            cluster_summaries = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Initialization failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    print(\"‚öñÔ∏è Running optimized GPT-4 pairwise judgments...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df_results = run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries)\n",
    "        df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "        elapsed = (time.time() - start_time) / 3600  # Convert to hours\n",
    "        print(f\"üíæ Saved {len(df_results)} comparisons to {OUTPUT_FILE}\")\n",
    "        print(f\"‚è±Ô∏è Total processing time: {elapsed:.2f} hours\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Judging process failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    print(\"‚úÖ Done. Results saved in:\")\n",
    "    print(f\"- Combined file: {OUTPUT_FILE}\")\n",
    "    print(\"- Pairwise comparison files in 'pairwise_results/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab387d3-0af6-48fd-8355-541b7f5d7135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07de35f4-b8ad-48c8-acd2-3cfb8e77291d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting GPU-accelerated evaluations...\n",
      "‚ö° Loading data with GPU acceleration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 78215.46it/s]\n",
      "GPU Evaluation:   0%|                                                                         | 0/90000 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Completed in 0.01 hours\n",
      "Results saved in: gpu_judge_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import ollama\n",
    "import time\n",
    "import logging\n",
    "import concurrent.futures\n",
    "import threading  # Added missing import\n",
    "from collections import defaultdict\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"\n",
    "OUTPUT_FOLDER = \"gpu_judge_results\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "MAX_GPU_WORKERS = 8  # Adjust based on your GPU capacity\n",
    "BATCH_SIZE = 500\n",
    "MODEL_CHUNK_SIZE = 4  # Models to evaluate simultaneously\n",
    "\n",
    "# High-capacity evaluation models\n",
    "EVALUATION_MODELS = [\n",
    "    'deepseek-llm:67b',\n",
    "    'llama3:70b',\n",
    "    'mixtral:8x22b',\n",
    "    'yi:34b',\n",
    "    'qwen:72b'\n",
    "]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='gpu_judging.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Optimized parallel data loading\"\"\"\n",
    "    print(\"‚ö° Loading data with GPU acceleration...\")\n",
    "    try:\n",
    "        # Parallel JSON loading\n",
    "        files = [f for f in os.listdir(RESULTS_FOLDER) if f.endswith(\"_results.json\")]\n",
    "        \n",
    "        def load_file(filename):\n",
    "            with open(os.path.join(RESULTS_FOLDER, filename)) as f:\n",
    "                model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                return model_name, json.load(f)\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(load_file, files), total=len(files), desc=\"Loading files\"))\n",
    "        \n",
    "        # Build response dict\n",
    "        response_dict = defaultdict(dict)\n",
    "        for model_name, entries in dict(results).items():\n",
    "            for entry in entries:\n",
    "                key = (entry[\"id\"], entry[\"condition\"])\n",
    "                response_dict[key][model_name] = entry[\"response\"]\n",
    "        \n",
    "        # Load other data\n",
    "        persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "        with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "            cluster_summaries = json.load(f)\n",
    "            \n",
    "        return dict(response_dict), persona_chat_df, cluster_summaries\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Data loading failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class GPUJudge:\n",
    "    def __init__(self):\n",
    "        self.model_cache = {}\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def evaluate_batch(self, batch):\n",
    "        \"\"\"Evaluate a batch of comparisons on GPU\"\"\"\n",
    "        results = []\n",
    "        for eval_model, id_, condition, model_a, model_b, response_a, response_b, persona, chat in batch:\n",
    "            try:\n",
    "                verdict = self._get_verdict(\n",
    "                    eval_model, persona, chat, \n",
    "                    response_a, response_b, model_a, model_b\n",
    "                )\n",
    "                results.append({\n",
    "                    \"id\": id_, \"condition\": condition,\n",
    "                    \"eval_model\": eval_model,\n",
    "                    \"model_a\": model_a, \"model_b\": model_b,\n",
    "                    \"verdict\": verdict,\n",
    "                    \"response_a\": response_a[:300],\n",
    "                    \"response_b\": response_b[:300]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed evaluation: {str(e)}\")\n",
    "        return results\n",
    "    \n",
    "    def _get_verdict(self, model_name, persona, chat, response_a, response_b, model_a, model_b):\n",
    "        \"\"\"Get verdict with model caching\"\"\"\n",
    "        with self.lock:\n",
    "            if model_name not in self.model_cache:\n",
    "                self._load_model(model_name)\n",
    "                \n",
    "        prompt = self._build_prompt(persona, chat, response_a, response_b, model_a, model_b)\n",
    "        \n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                response = ollama.generate(\n",
    "                    model=model_name,\n",
    "                    prompt=prompt,\n",
    "                    options={\n",
    "                        'temperature': 0.2,\n",
    "                        'num_ctx': 4096,\n",
    "                        'num_gpu': 1  # Ensure GPU utilization\n",
    "                    }\n",
    "                )\n",
    "                return self._parse_verdict(response['response'])\n",
    "            except Exception as e:\n",
    "                time.sleep(2 ** attempt)\n",
    "        \n",
    "        return \"ERROR\"\n",
    "    \n",
    "    def _load_model(self, model_name):\n",
    "        \"\"\"Load model with GPU optimization\"\"\"\n",
    "        try:\n",
    "            ollama.pull(model_name)\n",
    "            # Pre-warm model on GPU\n",
    "            ollama.generate(\n",
    "                model=model_name,\n",
    "                prompt=\"Warming up model...\",\n",
    "                options={'num_gpu': 1}\n",
    "            )\n",
    "            self.model_cache[model_name] = True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load {model_name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _build_prompt(self, persona, chat, response_a, response_b, model_a, model_b):\n",
    "        \"\"\"Optimized prompt for fast evaluation\"\"\"\n",
    "        return f\"\"\"Judge these responses (A vs B) based on:\n",
    "Persona: {persona[:200]}\n",
    "Chat: {chat[:200]}\n",
    "\n",
    "Model A: {response_a[:500]}\n",
    "Model B: {response_b[:500]}\n",
    "\n",
    "VERDICT (A/B/Tie):\"\"\"\n",
    "\n",
    "    def _parse_verdict(self, response):\n",
    "        \"\"\"Fast verdict parsing\"\"\"\n",
    "        verdict = response.strip().upper()\n",
    "        if verdict.startswith('A'): return 'A'\n",
    "        if verdict.startswith('B'): return 'B'\n",
    "        return 'Tie'\n",
    "\n",
    "def run_gpu_evaluations():\n",
    "    \"\"\"Ultra-fast GPU-optimized evaluation pipeline\"\"\"\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    judge = GPUJudge()\n",
    "    response_dict, persona_chat_df, cluster_summaries = load_data()\n",
    "    \n",
    "    # Prepare all evaluation tasks\n",
    "    tasks = []\n",
    "    for (id_, condition), model_responses in response_dict.items():\n",
    "        if len(model_responses) < 2:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "            chat = persona_chat_df.iloc[id_][\"chat\"]\n",
    "            \n",
    "            for model_a, model_b in combinations(model_responses.keys(), 2):\n",
    "                for eval_model in EVALUATION_MODELS:\n",
    "                    tasks.append((\n",
    "                        eval_model, id_, condition,\n",
    "                        model_a, model_b,\n",
    "                        model_responses[model_a], model_responses[model_b],\n",
    "                        persona, chat\n",
    "                    ))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Skipping id {id_}: {str(e)}\")\n",
    "    \n",
    "    # Process in parallel batches\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tqdm(total=len(tasks), desc=\"GPU Evaluation\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_GPU_WORKERS) as executor:\n",
    "            # Split tasks into model-specific chunks\n",
    "            task_batches = [tasks[i:i + MODEL_CHUNK_SIZE] for i in range(0, len(tasks), MODEL_CHUNK_SIZE)]\n",
    "            \n",
    "            futures = []\n",
    "            for batch in task_batches:\n",
    "                futures.append(executor.submit(judge.evaluate_batch, batch))\n",
    "                \n",
    "                # Process completed batches\n",
    "                if len(futures) >= MAX_GPU_WORKERS * 2:\n",
    "                    for future in concurrent.futures.as_completed(futures[:MAX_GPU_WORKERS]):\n",
    "                        results.extend(future.result())\n",
    "                        pbar.update(len(future.result()))\n",
    "                    \n",
    "                    # Save periodically\n",
    "                    if len(results) >= BATCH_SIZE:\n",
    "                        save_results(results)\n",
    "                        results = []\n",
    "                    \n",
    "                    futures = futures[MAX_GPU_WORKERS:]\n",
    "            \n",
    "            # Process remaining\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.extend(future.result())\n",
    "                pbar.update(len(future.result()))\n",
    "    \n",
    "    # Final save\n",
    "    save_results(results)\n",
    "    return time.time() - start_time\n",
    "\n",
    "def save_results(results):\n",
    "    \"\"\"Optimized results saving\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    timestamp = int(time.time())\n",
    "    df.to_csv(\n",
    "        os.path.join(OUTPUT_FOLDER, f\"results_{timestamp}.parquet\"),\n",
    "        index=False,\n",
    "        engine='pyarrow'  # Faster than CSV\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting GPU-accelerated evaluations...\")\n",
    "    try:\n",
    "        total_seconds = run_gpu_evaluations()\n",
    "        hours = total_seconds / 3600\n",
    "        print(f\"üèÅ Completed in {hours:.2f} hours\")\n",
    "        print(f\"Results saved in: {OUTPUT_FOLDER}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b7d5424-089f-4a35-b2f3-63538bc2e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting GPU-accelerated evaluations\n",
      "üîç Verifying Ollama connection...\n",
      "‚ùå Ollama verification failed: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "Please ensure:\n",
      "1. Ollama is running (ollama serve)\n",
      "2. Models are downloaded (ollama pull llama3:70b)\n",
      "‚ùå Evaluation failed - check logs for details\n",
      "üí• Critical failure: unsupported operand type(s) for /: 'NoneType' and 'int'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import ollama\n",
    "import time\n",
    "import logging\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"\n",
    "OUTPUT_FOLDER = \"gpu_judge_results\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "MAX_GPU_WORKERS = 4\n",
    "BATCH_SIZE = 100\n",
    "SAVE_INTERVAL = 300  # 5 minutes between saves\n",
    "\n",
    "# Evaluation models\n",
    "EVALUATION_MODELS = [\n",
    "    'llama3:70b',\n",
    "    'mixtral:8x22b'\n",
    "]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='gpu_judging.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def verify_ollama():\n",
    "    \"\"\"Verify Ollama is working with actual model test\"\"\"\n",
    "    try:\n",
    "        print(\"üîç Verifying Ollama connection...\")\n",
    "        test_response = ollama.generate(\n",
    "            model=EVALUATION_MODELS[0],\n",
    "            prompt=\"Test\",\n",
    "            options={'temperature': 0, 'num_gpu': 1}\n",
    "        )\n",
    "        if not test_response or 'response' not in test_response:\n",
    "            raise RuntimeError(\"Invalid Ollama response\")\n",
    "        print(\"‚úÖ Ollama verification successful\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ollama verification failed: {str(e)}\")\n",
    "        print(\"Please ensure:\")\n",
    "        print(\"1. Ollama is running (ollama serve)\")\n",
    "        print(\"2. Models are downloaded (ollama pull llama3:70b)\")\n",
    "        return False\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare evaluation data\"\"\"\n",
    "    print(\"üìÇ Loading data...\")\n",
    "    try:\n",
    "        all_data = {}\n",
    "        files = [f for f in os.listdir(RESULTS_FOLDER) if f.endswith(\"_results.json\")]\n",
    "        \n",
    "        for filename in tqdm(files, desc=\"Loading files\"):\n",
    "            try:\n",
    "                with open(os.path.join(RESULTS_FOLDER, filename)) as f:\n",
    "                    model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                    all_data[model_name] = json.load(f)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading {filename}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        response_dict = defaultdict(dict)\n",
    "        for model_name, entries in all_data.items():\n",
    "            for entry in entries:\n",
    "                key = (entry[\"id\"], entry[\"condition\"])\n",
    "                response_dict[key][model_name] = entry[\"response\"]\n",
    "\n",
    "        persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "        with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "            cluster_summaries = json.load(f)\n",
    "\n",
    "        return dict(response_dict), persona_chat_df, cluster_summaries\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Data loading failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class EvaluationEngine:\n",
    "    def __init__(self):\n",
    "        self.lock = threading.Lock()\n",
    "        self.completed = 0\n",
    "        self.last_save = time.time()\n",
    "        \n",
    "    def evaluate(self, task):\n",
    "        \"\"\"Perform a single evaluation\"\"\"\n",
    "        eval_model, id_, condition, model_a, model_b, response_a, response_b, persona, chat = task\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            prompt = self._build_prompt(persona, chat, response_a, response_b, model_a, model_b)\n",
    "            \n",
    "            response = ollama.generate(\n",
    "                model=eval_model,\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    'temperature': 0.2,\n",
    "                    'num_ctx': 4096,\n",
    "                    'num_gpu': 1,\n",
    "                    'timeout': 300  # 5 minute timeout\n",
    "                }\n",
    "            )\n",
    "            verdict = self._parse_verdict(response['response'])\n",
    "            eval_time = time.time() - start_time\n",
    "            \n",
    "            with self.lock:\n",
    "                self.completed += 1\n",
    "                if time.time() - self.last_save > SAVE_INTERVAL:\n",
    "                    logging.info(f\"Completed {self.completed} evals (last took {eval_time:.2f}s)\")\n",
    "                    self.last_save = time.time()\n",
    "            \n",
    "            return {\n",
    "                \"id\": id_,\n",
    "                \"condition\": condition,\n",
    "                \"eval_model\": eval_model,\n",
    "                \"model_a\": model_a,\n",
    "                \"model_b\": model_b,\n",
    "                \"verdict\": verdict,\n",
    "                \"eval_time\": eval_time,\n",
    "                \"response_a\": response_a[:500],\n",
    "                \"response_b\": response_b[:500]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed evaluation {id_}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _build_prompt(self, persona, chat, response_a, response_b, model_a, model_b):\n",
    "        \"\"\"Construct evaluation prompt\"\"\"\n",
    "        return f\"\"\"Evaluate these responses from {model_a} vs {model_b}:\n",
    "\n",
    "Persona: {persona[:200]}\n",
    "Chat: {chat[:200]}\n",
    "\n",
    "Response A:\n",
    "{response_a[:1000]}\n",
    "\n",
    "Response B:\n",
    "{response_b[:1000]}\n",
    "\n",
    "Which is better? Consider:\n",
    "1. Relevance to context\n",
    "2. Depth of insight\n",
    "3. Technical accuracy\n",
    "\n",
    "Respond ONLY with: A, B, or Tie\"\"\"\n",
    "    \n",
    "    def _parse_verdict(self, response):\n",
    "        \"\"\"Parse the judge's response\"\"\"\n",
    "        clean = response.strip().upper()\n",
    "        if clean.startswith('A'): return 'A'\n",
    "        if clean.startswith('B'): return 'B'\n",
    "        return 'Tie'\n",
    "\n",
    "def run_evaluations():\n",
    "    \"\"\"Main evaluation pipeline\"\"\"\n",
    "    if not verify_ollama():\n",
    "        return None  # Explicitly return None on failure\n",
    "    \n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    engine = EvaluationEngine()\n",
    "    \n",
    "    try:\n",
    "        response_dict, persona_chat_df, cluster_summaries = load_data()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load data: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare evaluation tasks\n",
    "    tasks = []\n",
    "    for (id_, condition), model_responses in response_dict.items():\n",
    "        if len(model_responses) < 2:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "            chat = persona_chat_df.iloc[id_][\"chat\"]\n",
    "            \n",
    "            for model_a, model_b in combinations(model_responses.keys(), 2):\n",
    "                for eval_model in EVALUATION_MODELS:\n",
    "                    tasks.append((\n",
    "                        eval_model, id_, condition,\n",
    "                        model_a, model_b,\n",
    "                        model_responses[model_a], model_responses[model_b],\n",
    "                        persona, chat\n",
    "                    ))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Skipping id {id_}: {str(e)}\")\n",
    "    \n",
    "    # Process evaluations\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tqdm(total=len(tasks), desc=\"Evaluating\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_GPU_WORKERS) as executor:\n",
    "            futures = {executor.submit(engine.evaluate, task): task for task in tasks}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                if len(results) >= BATCH_SIZE:\n",
    "                    save_results(results)\n",
    "                    results = []\n",
    "    \n",
    "    save_results(results)\n",
    "    return time.time() - start_time\n",
    "\n",
    "def save_results(results):\n",
    "    \"\"\"Save results to disk\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(results)\n",
    "        timestamp = int(time.time())\n",
    "        output_file = os.path.join(OUTPUT_FOLDER, f\"eval_results_{timestamp}.parquet\")\n",
    "        df.to_parquet(output_file, engine='pyarrow')\n",
    "        logging.info(f\"Saved {len(results)} results to {output_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save results: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting GPU-accelerated evaluations\")\n",
    "    try:\n",
    "        total_seconds = run_evaluations()\n",
    "        if total_seconds is None:\n",
    "            print(\"‚ùå Evaluation failed - check logs for details\")\n",
    "            exit(1)\n",
    "            \n",
    "        hours = total_seconds / 3600\n",
    "        print(f\"üèÅ Completed in {hours:.2f} hours\")\n",
    "        print(f\"Results saved in: {OUTPUT_FOLDER}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {str(e)}\")\n",
    "        print(f\"üí• Critical failure: {str(e)}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f6d7c-8611-467d-85a8-4fea1738d0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24172bc0-e81a-48ac-b1b1-c199a168c31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc367cbf-b6f8-47c5-a419-e61b486776f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994c8722-7667-4ce5-b7c0-f264a6f71e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data...\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 75\n",
      "‚öñÔ∏è Running DeepSeek pairwise judgments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [22:46:06<00:00, 91.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving combined results to new_results/deepseek_judge_comparisons.csv\n",
      "‚úÖ Done. Results saved in:\n",
      "- Combined file: new_results/deepseek_judge_comparisons.csv\n",
      "- Pairwise comparison files in 'new_results/pairwise_results' directory\n",
      "Total comparisons: 18000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import ollama\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"  # Folder containing *_results.json files\n",
    "OUTPUT_FILE = \"new_results/deepseek_judge_comparisons.csv\"\n",
    "PAIRWISE_DIR = \"new_results/pairwise_results\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "JUDGE_MODEL = \"deepseek-llm:67b\"\n",
    "\n",
    "# === Load Model Outputs ===\n",
    "def load_all_model_outputs(folder):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\"_results.json\"):\n",
    "            with open(os.path.join(folder, filename)) as f:\n",
    "                model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                data[model_name] = json.load(f)\n",
    "    return data\n",
    "\n",
    "# === Build Response Dict ===\n",
    "def build_response_dict(all_data):\n",
    "    response_dict = {}\n",
    "    for model_name, entries in all_data.items():\n",
    "        for entry in entries:\n",
    "            key = (entry[\"id\"], entry[\"condition\"])\n",
    "            if key not in response_dict:\n",
    "                response_dict[key] = {}\n",
    "            response_dict[key][model_name] = entry[\"response\"]\n",
    "    return response_dict\n",
    "\n",
    "# === DeepSeek Judge ===\n",
    "def deepseek_judge(persona, chat, response_a, response_b, model_a, model_b):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating two AI model responses.\n",
    "\n",
    "Persona: {persona}\n",
    "Chat Context: {chat}\n",
    "\n",
    "Model A ({model_a}):\n",
    "{response_a}\n",
    "\n",
    "Model B ({model_b}):\n",
    "{response_b}\n",
    "\n",
    "Who responded better? Answer with only one of: \\\"A\\\", \\\"B\\\", or \\\"Tie\\\".\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=JUDGE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during DeepSeek judge call: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "# === Judging Loop ===\n",
    "def run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries):\n",
    "    results = []\n",
    "    os.makedirs(PAIRWISE_DIR, exist_ok=True)\n",
    "\n",
    "    for (id_, condition), model_responses in tqdm(response_dict.items()):\n",
    "        if len(model_responses) < 2:\n",
    "            continue\n",
    "\n",
    "        persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "        try:\n",
    "            chat = persona_chat_df.loc[id_, \"chat\"]\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Skipping missing chat at id {id_}\")\n",
    "            continue\n",
    "\n",
    "        for model_a, model_b in combinations(model_responses.keys(), 2):\n",
    "            response_a = model_responses[model_a]\n",
    "            response_b = model_responses[model_b]\n",
    "\n",
    "            verdict = deepseek_judge(\n",
    "                persona=persona,\n",
    "                chat=chat,\n",
    "                response_a=response_a,\n",
    "                response_b=response_b,\n",
    "                model_a=model_a,\n",
    "                model_b=model_b,\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"id\": id_,\n",
    "                \"condition\": condition,\n",
    "                \"model_a\": model_a,\n",
    "                \"model_b\": model_b,\n",
    "                \"response_a\": response_a,\n",
    "                \"response_b\": response_b,\n",
    "                \"verdict\": verdict\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            pair_filename = f\"{PAIRWISE_DIR}/{model_a}_vs_{model_b}.csv\"\n",
    "            file_exists = os.path.exists(pair_filename)\n",
    "            pair_df = pd.DataFrame([result])\n",
    "            pair_df.to_csv(pair_filename, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üì• Loading data...\")\n",
    "    all_data = load_all_model_outputs(RESULTS_FOLDER)\n",
    "    response_dict = build_response_dict(all_data)\n",
    "    persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "    with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "        cluster_summaries = json.load(f)\n",
    "\n",
    "    print(\"‚öñÔ∏è Running DeepSeek pairwise judgments...\")\n",
    "    df_results = run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries)\n",
    "\n",
    "    print(f\"üíæ Saving combined results to {OUTPUT_FILE}\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"‚úÖ Done. Results saved in:\")\n",
    "    print(f\"- Combined file: {OUTPUT_FILE}\")\n",
    "    print(f\"- Pairwise comparison files in '{PAIRWISE_DIR}' directory\")\n",
    "    print(f\"Total comparisons: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8522fa-a6f6-48b2-a62c-42c4f5d53cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data...\n",
      "‚öñÔ∏è Running DeepSeek judgments (comparing gpt4 against all others)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [6:04:03<00:00, 24.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving combined results to new_results/deepseek_judge_comparisons2.csv\n",
      "‚úÖ Done. Results saved in:\n",
      "- Combined file: new_results/deepseek_judge_comparisons2.csv\n",
      "- Pairwise comparison files in 'new_results/pairwise_results' directory\n",
      "Total comparisons: 4872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"  # Folder containing *_results.json files\n",
    "OUTPUT_FILE = \"new_results/deepseek_judge_comparisons2.csv\"\n",
    "PAIRWISE_DIR = \"new_results/pairwise_results\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "JUDGE_MODEL = \"deepseek-llm:67b\"\n",
    "TARGET_MODEL = \"gpt4\"  # The model we'll compare against all others\n",
    "\n",
    "# === Load Model Outputs ===\n",
    "def load_all_model_outputs(folder):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\"_results.json\"):\n",
    "            with open(os.path.join(folder, filename)) as f:\n",
    "                model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                data[model_name] = json.load(f)\n",
    "    return data\n",
    "\n",
    "# === Build Response Dict ===\n",
    "def build_response_dict(all_data):\n",
    "    response_dict = {}\n",
    "    for model_name, entries in all_data.items():\n",
    "        for entry in entries:\n",
    "            key = (entry[\"id\"], entry[\"condition\"])\n",
    "            if key not in response_dict:\n",
    "                response_dict[key] = {}\n",
    "            response_dict[key][model_name] = entry[\"response\"]\n",
    "    return response_dict\n",
    "\n",
    "# === DeepSeek Judge ===\n",
    "def deepseek_judge(persona, chat, response_a, response_b, model_a, model_b):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating two AI model responses.\n",
    "\n",
    "Persona: {persona}\n",
    "Chat Context: {chat}\n",
    "\n",
    "Model A ({model_a}):\n",
    "{response_a}\n",
    "\n",
    "Model B ({model_b}):\n",
    "{response_b}\n",
    "\n",
    "Who responded better? Answer with only one of: \\\"A\\\", \\\"B\\\", or \\\"Tie\\\".\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=JUDGE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during DeepSeek judge call: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "# === Judging Loop ===\n",
    "def run_targeted_judging(response_dict, persona_chat_df, cluster_summaries):\n",
    "    results = []\n",
    "    os.makedirs(PAIRWISE_DIR, exist_ok=True)\n",
    "\n",
    "    for (id_, condition), model_responses in tqdm(response_dict.items()):\n",
    "        if TARGET_MODEL not in model_responses or len(model_responses) < 2:\n",
    "            continue  # Skip if target model isn't present or no other models to compare\n",
    "\n",
    "        persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "        try:\n",
    "            chat = persona_chat_df.loc[id_, \"chat\"]\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Skipping missing chat at id {id_}\")\n",
    "            continue\n",
    "\n",
    "        target_response = model_responses[TARGET_MODEL]\n",
    "        \n",
    "        # Compare target model against all other available models\n",
    "        for other_model in model_responses.keys():\n",
    "            if other_model == TARGET_MODEL:\n",
    "                continue\n",
    "                \n",
    "            other_response = model_responses[other_model]\n",
    "\n",
    "            verdict = deepseek_judge(\n",
    "                persona=persona,\n",
    "                chat=chat,\n",
    "                response_a=target_response,\n",
    "                response_b=other_response,\n",
    "                model_a=TARGET_MODEL,\n",
    "                model_b=other_model,\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"id\": id_,\n",
    "                \"condition\": condition,\n",
    "                \"model_a\": TARGET_MODEL,\n",
    "                \"model_b\": other_model,\n",
    "                \"response_a\": target_response,\n",
    "                \"response_b\": other_response,\n",
    "                \"verdict\": verdict\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            pair_filename = f\"{PAIRWISE_DIR}/{TARGET_MODEL}_vs_{other_model}.csv\"\n",
    "            file_exists = os.path.exists(pair_filename)\n",
    "            pair_df = pd.DataFrame([result])\n",
    "            pair_df.to_csv(pair_filename, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üì• Loading data...\")\n",
    "    all_data = load_all_model_outputs(RESULTS_FOLDER)\n",
    "    response_dict = build_response_dict(all_data)\n",
    "    persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "    with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "        cluster_summaries = json.load(f)\n",
    "\n",
    "    print(f\"‚öñÔ∏è Running DeepSeek judgments (comparing {TARGET_MODEL} against all others)...\")\n",
    "    df_results = run_targeted_judging(response_dict, persona_chat_df, cluster_summaries)\n",
    "\n",
    "    print(f\"üíæ Saving combined results to {OUTPUT_FILE}\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"‚úÖ Done. Results saved in:\")\n",
    "    print(f\"- Combined file: {OUTPUT_FILE}\")\n",
    "    print(f\"- Pairwise comparison files in '{PAIRWISE_DIR}' directory\")\n",
    "    print(f\"Total comparisons: {len(df_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b512236-fca8-4cd1-9e88-65ef45eaeef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data...\n",
      "‚öñÔ∏è Running gemma pairwise judgments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [6:52:12<00:00, 27.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving combined results to new_results/gemma/gemma_judge_comparisons.csv\n",
      "‚úÖ Done. Results saved in:\n",
      "- Combined file: new_results/gemma/gemma_judge_comparisons.csv\n",
      "- Pairwise comparison files in 'new_results/gemma/pairwise_results' directory\n",
      "Total comparisons: 22872\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import ollama\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"  # Folder containing *_results.json files\n",
    "OUTPUT_FILE = \"new_results/gemma/gemma_judge_comparisons.csv\"\n",
    "PAIRWISE_DIR = \"new_results/gemma/pairwise_results\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "JUDGE_MODEL = \"gemma3:27b\"\n",
    "\n",
    "# === Load Model Outputs ===\n",
    "def load_all_model_outputs(folder):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\"_results.json\"):\n",
    "            with open(os.path.join(folder, filename)) as f:\n",
    "                model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                data[model_name] = json.load(f)\n",
    "    return data\n",
    "\n",
    "# === Build Response Dict ===\n",
    "def build_response_dict(all_data):\n",
    "    response_dict = {}\n",
    "    for model_name, entries in all_data.items():\n",
    "        for entry in entries:\n",
    "            key = (entry[\"id\"], entry[\"condition\"])\n",
    "            if key not in response_dict:\n",
    "                response_dict[key] = {}\n",
    "            response_dict[key][model_name] = entry[\"response\"]\n",
    "    return response_dict\n",
    "\n",
    "# === DeepSeek Judge ===\n",
    "def deepseek_judge(persona, chat, response_a, response_b, model_a, model_b):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating two AI model responses.\n",
    "\n",
    "Persona: {persona}\n",
    "Chat Context: {chat}\n",
    "\n",
    "Model A ({model_a}):\n",
    "{response_a}\n",
    "\n",
    "Model B ({model_b}):\n",
    "{response_b}\n",
    "\n",
    "Who responded better? Answer with only one of: \\\"A\\\", \\\"B\\\", or \\\"Tie\\\".\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=JUDGE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during DeepSeek judge call: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "# === Judging Loop ===\n",
    "def run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries):\n",
    "    results = []\n",
    "    os.makedirs(PAIRWISE_DIR, exist_ok=True)\n",
    "\n",
    "    for (id_, condition), model_responses in tqdm(response_dict.items()):\n",
    "        if len(model_responses) < 2:\n",
    "            continue\n",
    "\n",
    "        persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "        try:\n",
    "            chat = persona_chat_df.loc[id_, \"chat\"]\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Skipping missing chat at id {id_}\")\n",
    "            continue\n",
    "\n",
    "        for model_a, model_b in combinations(model_responses.keys(), 2):\n",
    "            response_a = model_responses[model_a]\n",
    "            response_b = model_responses[model_b]\n",
    "\n",
    "            verdict = deepseek_judge(\n",
    "                persona=persona,\n",
    "                chat=chat,\n",
    "                response_a=response_a,\n",
    "                response_b=response_b,\n",
    "                model_a=model_a,\n",
    "                model_b=model_b,\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"id\": id_,\n",
    "                \"condition\": condition,\n",
    "                \"model_a\": model_a,\n",
    "                \"model_b\": model_b,\n",
    "                \"response_a\": response_a,\n",
    "                \"response_b\": response_b,\n",
    "                \"verdict\": verdict\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            pair_filename = f\"{PAIRWISE_DIR}/{model_a}_vs_{model_b}.csv\"\n",
    "            file_exists = os.path.exists(pair_filename)\n",
    "            pair_df = pd.DataFrame([result])\n",
    "            pair_df.to_csv(pair_filename, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üì• Loading data...\")\n",
    "    all_data = load_all_model_outputs(RESULTS_FOLDER)\n",
    "    response_dict = build_response_dict(all_data)\n",
    "    persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "    with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "        cluster_summaries = json.load(f)\n",
    "\n",
    "    print(\"‚öñÔ∏è Running gemma pairwise judgments...\")\n",
    "    df_results = run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries)\n",
    "\n",
    "    print(f\"üíæ Saving combined results to {OUTPUT_FILE}\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"‚úÖ Done. Results saved in:\")\n",
    "    print(f\"- Combined file: {OUTPUT_FILE}\")\n",
    "    print(f\"- Pairwise comparison files in '{PAIRWISE_DIR}' directory\")\n",
    "    print(f\"Total comparisons: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e1e80a-282b-405b-af95-b47d1c3a0c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data...\n",
      "‚öñÔ∏è Running gwen3 pairwise judgments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [35:49:19<00:00, 143.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving combined results to new_results/gwen3/gemma_judge_comparisons.csv\n",
      "‚úÖ Done. Results saved in:\n",
      "- Combined file: new_results/gwen3/gemma_judge_comparisons.csv\n",
      "- Pairwise comparison files in 'new_results/gwen3/pairwise_results' directory\n",
      "Total comparisons: 17979\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import ollama\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"  # Folder containing *_results.json files\n",
    "OUTPUT_FILE = \"new_results/gwen3/gemma_judge_comparisons.csv\"\n",
    "PAIRWISE_DIR = \"new_results/gwen3/pairwise_results\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "JUDGE_MODEL = \"qwen3:latest\"\n",
    "\n",
    "# === Load Model Outputs ===\n",
    "def load_all_model_outputs(folder):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\"_results.json\"):\n",
    "            with open(os.path.join(folder, filename)) as f:\n",
    "                model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                data[model_name] = json.load(f)\n",
    "    return data\n",
    "\n",
    "# === Build Response Dict ===\n",
    "def build_response_dict(all_data):\n",
    "    response_dict = {}\n",
    "    for model_name, entries in all_data.items():\n",
    "        for entry in entries:\n",
    "            key = (entry[\"id\"], entry[\"condition\"])\n",
    "            if key not in response_dict:\n",
    "                response_dict[key] = {}\n",
    "            response_dict[key][model_name] = entry[\"response\"]\n",
    "    return response_dict\n",
    "\n",
    "# === DeepSeek Judge ===\n",
    "def deepseek_judge(persona, chat, response_a, response_b, model_a, model_b):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating two AI model responses.\n",
    "\n",
    "Persona: {persona}\n",
    "Chat Context: {chat}\n",
    "\n",
    "Model A ({model_a}):\n",
    "{response_a}\n",
    "\n",
    "Model B ({model_b}):\n",
    "{response_b}\n",
    "\n",
    "Who responded better? Answer with only one of: \\\"A\\\", \\\"B\\\", or \\\"Tie\\\".\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=JUDGE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during gwen3 judge call: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "# === Judging Loop ===\n",
    "def run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries):\n",
    "    results = []\n",
    "    os.makedirs(PAIRWISE_DIR, exist_ok=True)\n",
    "\n",
    "    for (id_, condition), model_responses in tqdm(response_dict.items()):\n",
    "        if len(model_responses) < 2:\n",
    "            continue\n",
    "\n",
    "        persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "        try:\n",
    "            chat = persona_chat_df.loc[id_, \"chat\"]\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Skipping missing chat at id {id_}\")\n",
    "            continue\n",
    "\n",
    "        for model_a, model_b in combinations(model_responses.keys(), 2):\n",
    "            response_a = model_responses[model_a]\n",
    "            response_b = model_responses[model_b]\n",
    "\n",
    "            verdict = deepseek_judge(\n",
    "                persona=persona,\n",
    "                chat=chat,\n",
    "                response_a=response_a,\n",
    "                response_b=response_b,\n",
    "                model_a=model_a,\n",
    "                model_b=model_b,\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"id\": id_,\n",
    "                \"condition\": condition,\n",
    "                \"model_a\": model_a,\n",
    "                \"model_b\": model_b,\n",
    "                \"response_a\": response_a,\n",
    "                \"response_b\": response_b,\n",
    "                \"verdict\": verdict\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            pair_filename = f\"{PAIRWISE_DIR}/{model_a}_vs_{model_b}.csv\"\n",
    "            file_exists = os.path.exists(pair_filename)\n",
    "            pair_df = pd.DataFrame([result])\n",
    "            pair_df.to_csv(pair_filename, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üì• Loading data...\")\n",
    "    all_data = load_all_model_outputs(RESULTS_FOLDER)\n",
    "    response_dict = build_response_dict(all_data)\n",
    "    persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "    with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "        cluster_summaries = json.load(f)\n",
    "\n",
    "    print(\"‚öñÔ∏è Running gwen3 pairwise judgments...\")\n",
    "    df_results = run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries)\n",
    "\n",
    "    print(f\"üíæ Saving combined results to {OUTPUT_FILE}\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"‚úÖ Done. Results saved in:\")\n",
    "    print(f\"- Combined file: {OUTPUT_FILE}\")\n",
    "    print(f\"- Pairwise comparison files in '{PAIRWISE_DIR}' directory\")\n",
    "    print(f\"Total comparisons: {len(df_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830684be-dca1-42b4-b579-2c848bef1b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data...\n",
      "‚öñÔ∏è Running gpt pairwise judgments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 651/900 [27:02:11<16:41:36, 241.35s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import ollama\n",
    "\n",
    "# === CONFIG ===\n",
    "RESULTS_FOLDER = \"./\"  # Folder containing *_results.json files\n",
    "OUTPUT_FILE = \"new_results/gpt/gemma_judge_comparisons.csv\"\n",
    "PAIRWISE_DIR = \"new_results/gpt/pairwise_results\"\n",
    "PERSONA_CHAT_CSV = \"personality.csv\"\n",
    "CLUSTER_SUMMARIES_JSON = \"cluster_persona_summaries.json\"\n",
    "JUDGE_MODEL = \"gpt-oss:latest\"\n",
    "\n",
    "# === Load Model Outputs ===\n",
    "def load_all_model_outputs(folder):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\"_results.json\"):\n",
    "            with open(os.path.join(folder, filename)) as f:\n",
    "                model_name = filename.replace(\"ollama_\", \"\").replace(\"_results.json\", \"\")\n",
    "                data[model_name] = json.load(f)\n",
    "    return data\n",
    "\n",
    "# === Build Response Dict ===\n",
    "def build_response_dict(all_data):\n",
    "    response_dict = {}\n",
    "    for model_name, entries in all_data.items():\n",
    "        for entry in entries:\n",
    "            key = (entry[\"id\"], entry[\"condition\"])\n",
    "            if key not in response_dict:\n",
    "                response_dict[key] = {}\n",
    "            response_dict[key][model_name] = entry[\"response\"]\n",
    "    return response_dict\n",
    "\n",
    "# === DeepSeek Judge ===\n",
    "def deepseek_judge(persona, chat, response_a, response_b, model_a, model_b):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating two AI model responses.\n",
    "\n",
    "Persona: {persona}\n",
    "Chat Context: {chat}\n",
    "\n",
    "Model A ({model_a}):\n",
    "{response_a}\n",
    "\n",
    "Model B ({model_b}):\n",
    "{response_b}\n",
    "\n",
    "Who responded better? Answer with only one of: \\\"A\\\", \\\"B\\\", or \\\"Tie\\\".\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=JUDGE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during gwen3 judge call: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "# === Judging Loop ===\n",
    "def run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries):\n",
    "    results = []\n",
    "    os.makedirs(PAIRWISE_DIR, exist_ok=True)\n",
    "\n",
    "    for (id_, condition), model_responses in tqdm(response_dict.items()):\n",
    "        if len(model_responses) < 2:\n",
    "            continue\n",
    "\n",
    "        persona = cluster_summaries.get(str(id_ % 10), \"\")\n",
    "        try:\n",
    "            chat = persona_chat_df.loc[id_, \"chat\"]\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Skipping missing chat at id {id_}\")\n",
    "            continue\n",
    "\n",
    "        for model_a, model_b in combinations(model_responses.keys(), 2):\n",
    "            response_a = model_responses[model_a]\n",
    "            response_b = model_responses[model_b]\n",
    "\n",
    "            verdict = deepseek_judge(\n",
    "                persona=persona,\n",
    "                chat=chat,\n",
    "                response_a=response_a,\n",
    "                response_b=response_b,\n",
    "                model_a=model_a,\n",
    "                model_b=model_b,\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"id\": id_,\n",
    "                \"condition\": condition,\n",
    "                \"model_a\": model_a,\n",
    "                \"model_b\": model_b,\n",
    "                \"response_a\": response_a,\n",
    "                \"response_b\": response_b,\n",
    "                \"verdict\": verdict\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            pair_filename = f\"{PAIRWISE_DIR}/{model_a}_vs_{model_b}.csv\"\n",
    "            file_exists = os.path.exists(pair_filename)\n",
    "            pair_df = pd.DataFrame([result])\n",
    "            pair_df.to_csv(pair_filename, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üì• Loading data...\")\n",
    "    all_data = load_all_model_outputs(RESULTS_FOLDER)\n",
    "    response_dict = build_response_dict(all_data)\n",
    "    persona_chat_df = pd.read_csv(PERSONA_CHAT_CSV)\n",
    "    with open(CLUSTER_SUMMARIES_JSON) as f:\n",
    "        cluster_summaries = json.load(f)\n",
    "\n",
    "    print(\"‚öñÔ∏è Running gpt pairwise judgments...\")\n",
    "    df_results = run_pairwise_judging(response_dict, persona_chat_df, cluster_summaries)\n",
    "\n",
    "    print(f\"üíæ Saving combined results to {OUTPUT_FILE}\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"‚úÖ Done. Results saved in:\")\n",
    "    print(f\"- Combined file: {OUTPUT_FILE}\")\n",
    "    print(f\"- Pairwise comparison files in '{PAIRWISE_DIR}' directory\")\n",
    "    print(f\"Total comparisons: {len(df_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114fe06-10d1-4411-9b4b-d1a0af15d3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-env)",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
